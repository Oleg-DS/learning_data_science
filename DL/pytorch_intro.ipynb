{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Вступление\n",
    "\n",
    "Всем привет! На сегодняшнем семинаре мы познакомимся с библиотекой **PyTorch**. Он очень похож на Numpy, с одним лишь отличием (на самом деле их больше, но сейчас мы поговорим про самое главное) — PyTorch может считать градиенты за вас. Таким образом, вам не надо будет руками писать обратный проход в нейросетях.\n",
    "\n",
    "#### Семинар построен следующим образом:\n",
    "\n",
    "1. Вспоминаем Numpy и сравниваем операции в PyTorch\n",
    "2. Создаем тензоры в PyTorch\n",
    "3. Работаем с градиентами руками\n",
    "4. Моя первая нейросеть "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Вспоминаем Numpy и сравниваем операции в PyTorch\n",
    "\n",
    "Мы можем создавать матрицы, перемножать их, складывать, транспонировать и в целом совершать любые матричные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5, 3) # создали случайную матрицу \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Проверили размеры: {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Добавили 5:\\n{a + 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X*X^T:\\n{a @ a.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Среднее по колонкам:\\n{a.mean(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Изменили размеры: {a.reshape(3, 5).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разминка.\n",
    "\n",
    "При помощи numpy посчитайте сумму квадратов натуральных чисел от 1 до 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные операции в **PyTorch** выглядят следующим образом, синтаксис почти не отличается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Проверили размеры: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Добавили 5:\\n{x + 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X*X^T:\\n{x @ x.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Среднее по колонкам:\\n{x.mean(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Изменили размеры:\\n{x.reshape([3, 5]).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшой пример того, как меняются операции:\n",
    "\n",
    "* `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* `x.astype(np.int64) -> x.type(torch.int64)`\n",
    "\n",
    "Для помощи вам есть [таблица](https://pytorch-for-numpy-users.wkentaro.com/), которая поможет вам найти аналог операции в Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем тензоры в PyTorch и снова изучаем базовые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)  # пустой тензор (т.е. без инициализации)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)  # тензор со случайными числами\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.int64)  # тензор с нулями и указанием типов чисел\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])  # конструируем тензор из питоновского листа\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float32)  # создаем матрицу с размерами как у x\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "z = x + y  # операция сложения\n",
    "print(z)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.zero_()  # зануление значений тензора\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x * y)  # поэлементное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x @ y.T)  # матричное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.unsqueeze(0).shape)  # добавили измерение в начало, аналог броадкастинга "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.unsqueeze(0).squeeze(0).shape)  # убрали измерение в начале, аналог броадкастинга "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем делать обычные срезы и переводить матрицы назад в numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((3, 5))\n",
    "x = torch.ones((3, 5))\n",
    "print(np.allclose(x.numpy(), a))\n",
    "print(np.allclose(x.numpy()[:, 1], a[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Руками учим линейную регрессию на Numpy и на PyTorch\n",
    "\n",
    "Для примера возьмём датасет [Boston house prices](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset), а точнее, его последнюю колонку (\"Median value of owner-occupied homes in $1000’s\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Через Numpy\n",
    "\n",
    "Для сравнения реализуем линейную регрессию на чистом Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "x = boston.data[:, -1] / boston.data[:, -1].max()\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним основные формулы линейной регрессии. Предсказание $\\hat y$ и лосс $L$ описываются так:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat y &= X \\cdot w + b \\\\\n",
    "L(y, \\hat y) &= \\frac 1 n \\sum_{i = 1}^n (y_i - \\hat {y_i})^2 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Производная лосса $L$ по предсказаниям $\\hat {y_i}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {\\partial L} {\\partial \\hat {y_i}} (y, \\hat y) &= \\frac 2 n (\\hat {y_i} - y_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Производная предсказаний $\\hat {y_i}$ по параметрам $w$, $b$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {\\partial \\hat {y_i}} {\\partial w} &= x_i \\\\\n",
    "\\frac {\\partial \\hat {y_i}} {\\partial b} &= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Производная лосса $L$ по параметрам $w$, $b$ (chain rule):\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {\\partial L} {\\partial w} (y, \\hat y) &= \\sum_{i = 1}^n \\frac 2 n (\\hat {y_i} - y_i) x_i \\\\\n",
    "\\frac {\\partial L} {\\partial b} (y, \\hat y) &= \\sum_{i = 1}^n \\frac 2 n (\\hat {y_i} - y_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Напишем функцию, которая по тому, что мы вычисляем во время предсказания, вернёт производные лосса по параметрам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(w, b, x, y, y_pred, loss):\n",
    "    w_grad = <YOUR CODE>\n",
    "    b_grad = <YOUR CODE>\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = <YOUR CODE>\n",
    "loss = <YOUR CODE>\n",
    "w_grad, b_grad = get_grad(w, b, x, y, y_pred, loss)\n",
    "\n",
    "print(f\"dL/dw = {w_grad}\")\n",
    "print(f\"dL/db = {b_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательная функция, чтобы рисовать графики во время обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def log_output(x, y, y_pred, i, loss):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x, y)\n",
    "        plt.scatter(x, y_pred, color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"[Iteration {i}] loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 400\n",
    "\n",
    "for i in range(num_iters):\n",
    "    y_pred = <YOUR CODE>\n",
    "    loss = <YOUR CODE>\n",
    "    \n",
    "    w_grad, b_grad = get_grad(w, b, x, y, y_pred, loss)\n",
    "\n",
    "    # делаем шаг градиентного спуска с lr = 0.05\n",
    "    w -= <YOUR CODE>\n",
    "    b -= <YOUR CODE>\n",
    "\n",
    "    log_output(x, y, y_pred, i, loss)\n",
    "    \n",
    "    if loss < 0.5:\n",
    "        print(\"Done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Через PyTorch\n",
    "\n",
    "Сразу сконвертируем датасет в `torch.Tensor`, чтобы об этом дальше не думать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(boston.data[:, -1] / boston.data[:, -1].max(), dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть возможность при создании тензора указывать, нужно ли считать по нему градиент или нет, с помощью параметра `requires_grad`. Когда `requires_grad=True` мы сообщаем фреймворку, о том, что мы хотим следить за всеми тензорами, которые получаются из созданного. Иными словами, у любого тензора, у которого указан данный параметр, будет доступ к цепочке операций и преобразований совершенными с ними. Если эти функции дифференцируемые, то у тензора появляется параметр `.grad`, в котором хранится значение градиента.\n",
    "\n",
    "Если к тензору, получающемуся в результате, применить метод `.backward()`, то фреймворк посчитает по цепочке градиенту для всех тензоров, у которых `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "assert w.grad is None # только создали тензоры и в них нет градиентов\n",
    "assert b.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# совершаем операции с тензорами\n",
    "y_pred = <YOUR CODE>\n",
    "loss = <YOUR CODE>  # подсказка: используйте torch.mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на `grad_fn` у `loss`. Наличие этого атрибута говорит о том, что тензор является частью вычислительного графа, а последней операцией, совершённой с этим тензором, был `mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # считаем градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert w.grad is not None  # сделали операции и посчитали градиенты, значение должно было появиться\n",
    "assert b.grad is not None\n",
    "\n",
    "assert isinstance(w.grad, torch.Tensor)  # градиент — это тоже тензор\n",
    "assert isinstance(b.grad, torch.Tensor)\n",
    "\n",
    "print(f\"dL/dw = {w.grad}\")\n",
    "print(f\"dL/db = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 400\n",
    "\n",
    "for i in range(num_iters):\n",
    "    y_pred = <YOUR CODE>\n",
    "    loss = <YOUR CODE>\n",
    "    loss.backward()\n",
    "\n",
    "    # отключаем вычисление градиентов на то время, пока мы руками лезем в .grad\n",
    "    with torch.no_grad():\n",
    "        # делаем шаг градиентного спуска с lr = 0.05\n",
    "        w -= <YOUR CODE>\n",
    "        b -= <YOUR CODE>\n",
    "\n",
    "        # обнуляем градиенты, чтобы на следующем шаге опять посчитать и не аккумулировать их\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    # PyTorch запрещает вызывать .numpy() на тензорах, у которых requires_grad=True, поэтому\n",
    "    # вначале делаем копию тензора при помощи .detach()\n",
    "    log_output(x.numpy(), y.numpy(), y_pred.detach().numpy(), i, loss.detach().numpy())\n",
    "    \n",
    "    if loss.detach().numpy() < 0.5:\n",
    "        print(\"Done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 `torch.optim`, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть много разных инструментов для вычисления и оптимизации функций. Здесь мы познакомимся с двумя:\n",
    "\n",
    "* Оптимизаторы. Они все лежат в `torch.optim.*`: например, `torch.optim.SGD`, `torch.optim.Adam`, etc.\n",
    "* Criterions, они же лоссы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизатор создаётся так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD([w, b], lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оптимизатора реализует те две операции, которые мы раньше делали руками:\n",
    "\n",
    "* Обновление параметров: `opt.step()`\n",
    "* Зануление сохранённых градиентов: `opt.zero_grad()`\n",
    "\n",
    "Criterion — это штука, которая считает лосс. У них есть два разных интерфейса: объектно-ориентированный (`torch.nn.*`) и функциональный (`torch.nn.functional.*`). Пользоваться объектно-ориентированным интерфейсом можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "y_pred = w * x + b\n",
    "print(criterion(y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но с функциональным интерфейсом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.mse_loss(y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим всё это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 400\n",
    "\n",
    "for i in range(num_iters):\n",
    "    y_pred = <YOUR CODE>\n",
    "    loss = <YOUR CODE>\n",
    "    loss.backward()\n",
    "    \n",
    "    # Обновите параметры при помощи только что посчитанных градиентов...\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    # ...и занулите тензоры с градиентами\n",
    "    <YOUR CODE>\n",
    "\n",
    "    log_output(x.numpy(), y.numpy(), y_pred.detach().numpy(), i, loss.detach().numpy())\n",
    "    \n",
    "    if loss.detach().numpy() < 0.5:\n",
    "        print(\"Done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 `torch.utils.data.{Dataset,DataLoader}`\n",
    "\n",
    "Чтобы в PyTorch иметь возможность итерироваться по данным и применять к ним преобразования, например, аугментации, о которых вы узнаете позже, нужно создать свой класс, унаследованный от `torch.utils.data.Dataset`.\n",
    "\n",
    "Вот пример из документации:\n",
    "\n",
    "```python\n",
    "class FaceLandmarksDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "```\n",
    "\n",
    "Как вы видите, у такого класса должно быть два метода: \n",
    "\n",
    "* `__len__`: возвращает информацию о том, сколько объектов у нас в датасете\n",
    "* `__getitem__`: возвращает семпл и таргет к нему\n",
    "\n",
    "\n",
    "Теперь давайте напишем такой сами. В качестве датасета снова возьмём последнюю колонку из Boston house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Our dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sample': torch.tensor(self.x[idx], dtype=torch.float32),\n",
    "            'target': torch.tensor(self.y[idx], dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset = OurDataset(x=boston.data[:, -1] / boston.data[:, -1].max(), y=boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset[1]  # [1] под капотом вызывает .__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам хотелось бы избежать необходимости во время обучения нейросети целиком прогонять через неё все примеры из обучающей выборки одновременно. Для этого датасет нарезают на батчи — кусочки по несколько элементов — и прогоняют их через нейросеть по одному. Сделать это на чистом питоне можно примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for start_idx in range(0, len(our_dataset), batch_size):\n",
    "    batch = our_dataset[start_idx:start_idx + batch_size]\n",
    "    batch_x = batch['sample']\n",
    "    batch_y = batch['target']\n",
    "    print('Sample:', batch_x)\n",
    "    print('Target:', batch_y)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch такую функциональность предоставляет класс `DataLoader`. Вообще, он умеет делать много чего помимо этого, но нас сейчас интересует именно его способность резать датасет на батчи. Пользоваться им можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(our_dataset, batch_size=64)\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch_x = batch['sample']\n",
    "    batch_y = batch['target']\n",
    "    print('Sample:', batch_x)\n",
    "    print('Target:', batch_y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся этим даталоадером и проитерируемся по датасету батчами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "opt = torch.optim.SGD([w, b], lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200\n",
    "\n",
    "for i in range(num_iters):\n",
    "    for batch in dataloader:\n",
    "        x_batch = <YOUR CODE>\n",
    "        y_batch = <YOUR CODE>\n",
    "        y_pred = w * x_batch + b\n",
    "        loss = F.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Чтобы нарисовать график, здесь мы всё равно прогоним весь датасет целиком через линейную регрессию.\n",
    "    # С большими датасетами и большими моделями это не получилось бы, и пришлось бы собирать метрики, прогоняя\n",
    "    # датасет через модель батчами.\n",
    "    y_pred = w * x + b\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "    log_output(x.numpy(), y.numpy(), y_pred.detach().numpy(), i, loss.detach().numpy())\n",
    "    \n",
    "    if loss.detach().numpy() < 0.5:\n",
    "        print(\"Done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Менее игрушечный пример\n",
    "\n",
    "## 4.1 Скачиваем датасет\n",
    "\n",
    "Возьмём чуть более серьёзный датасет, чем последняя колонка Boston house prices. Например, MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы сайт, на котором выложен датасет, не принял нас за ботов, прикинемся браузером\n",
    "\n",
    "import urllib\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.hub import _get_torch_home\n",
    "\n",
    "# На Linux датасет скачается в ~/.cache/torch/datasets, но можете выбрать любую другую папку\n",
    "mnist_path = Path(_get_torch_home()) / 'datasets'\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    mnist_path, train=True, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ") # используем готовый класс от торча для загрузки данных для тренировки\n",
    "mnist_valid = torchvision.datasets.MNIST(\n",
    "    mnist_path, train=False, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ") # используем готовый класс от торча для загрузки данных для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "for i in range(n):\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(mnist_train[i][0].squeeze(0).numpy().reshape([28, 28]), cmap='gray')\n",
    "    plt.title(str(mnist_train[i][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет довольно большой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnist_train), len(mnist_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведём даталоадеры для MNIST. Параметр `num_workers=1` означает, что даталоадер создаст 1 дочерний процесс, который будет заниматься в фоне загрузкой датасета в память и заполнением очереди из батчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_train, batch_size=4, shuffle=True, num_workers=1\n",
    ")\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_valid, batch_size=4, shuffle=False, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Собираем нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае в PyTorch для создания нейросетей используется модуль `nn`. Нейросеть должна быть унаследована от класса `nn.Module`. Пример, как это может выглядеть:\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 30)\n",
    "        self.fc2 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "```\n",
    "\n",
    "Как мы видим на этом примере, у данного класса должно быть метод `forward`, который определяет прямой проход нейросети. Также из класса выше видно, что модуль `nn` содержит в себе реализацию большинства слоев, а модуль `nn.functional` -- функций активаций.\n",
    "\n",
    "Есть еще один способ создать нейросеть, если она представляет собой последовательное применение слоёв. Разберем его на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(  # создаем контейнер, который инициализируем списком слоёв\n",
    "    nn.Linear(5, 3),    # добавили слой с 5-ю нейронами на вход и 3-мя на выход\n",
    "    nn.ReLU(),          # добавили функцию активации\n",
    "    nn.Linear(3, 1),    # добавили слой с 3-мя нейронами на вход и 5-ю на выход\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = torch.randn((2, 5))\n",
    "y_pred = model(batch_x) # получили предсказания модели\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на `grad_fn`. Тензор `y_pred` помнит про то, что последней операцией в его вычислительном графе была [`addmm`](https://pytorch.org/docs/stable/generated/torch.addmm.html), то есть (упрощая) `b + m @ x`. Это соответствует `nn.Linear` в конце модели!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберём теперь модель, подходящую для MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),         # превращаем картинку 28х28 в вектор размером 784\n",
    "    nn.Linear(784, 128),  # входной слой размером 784 нейронов с выходом в 128 нейронов\n",
    "    nn.ReLU(),            # функция активации релу\n",
    "    nn.Linear(128, 10),   # ещё один линейный слой\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите внимательно: мы сейчас будем заниматься классификацией, но в конце модели нет никакого софтмакса! Как так?\n",
    "\n",
    "Дело в том, что поскольку во время обучения мы будем оптимизировать negative log likelihood, после softmax в функции потерь будет сразу стоять логарифм. Последовательное вычисление сначала softmax, а потом его логарифма может приводить к большим ошибкам округления, поэтому обычно эти две функции соединяют в композицию `log_softmax`, которая ведёт себя гораздо лучше:\n",
    "\n",
    "$$\n",
    "\\log \\left[ \\operatorname{softmax}(x) \\right]_i =\n",
    "\\log \\left( \\frac {\\exp(x_i)} {\\sum_{j=1}^n \\exp(x_j)} \\right) =\n",
    "x_i - \\log\\left( \\sum_{j=1}^n \\exp(x_j) \\right) =\n",
    "x_i - \\log\\left( \\sum_{j=1}^n \\exp(x_j - x_* + x_*) \\right) =\n",
    "x_i - x_* - \\log\\left( \\sum_{j=1}^n \\exp(x_j - x_*) \\right),\n",
    "$$\n",
    "\n",
    "где $x_* = \\max \\left\\{ x_i \\right\\}$.\n",
    "\n",
    "Поэтому мы можем:\n",
    "\n",
    "* Либо поставить на выход модели функцию активации `log_softmax` и учить её с функцией потерь negative log likelihood (в PyTorch она называется `nn.NLLLoss` или `F.nll_loss`),\n",
    "* Либо оставить модель безо всякой функции активации в конце и учить её с функцией потерь `nll_loss(log_softmax())`. В PyTorch такая композитная функция потерь называется `nn.CrossEntropyLoss` или `F.cross_entropy`, ей мы и воспользуемся.\n",
    "\n",
    "А когда мы захотим предсказать классы, будет достаточно просто посчитать `argmax(model(), dim=-1).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Training loop\n",
    "\n",
    "Веса моделей хранятся в виде матриц и выглядят так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in model.parameters()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.05) # создаем оптимизатор и передаем туда параметры модели\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):  # всего у нас будет 10 эпох (10 раз подряд пройдемся по всем батчам из трейна)\n",
    "    # Трейн\n",
    "    for x_batch, y_batch in tqdm(train_dataloader, desc=f'Epoch {epoch} | Train'):\n",
    "        y_pred = model(x_batch) # делаем предсказания\n",
    "        loss = criterion(y_pred, y_batch) # считаем лосс\n",
    "        \n",
    "        ############################## Собственно обучение ##############################\n",
    "        # 1. Считаем градиенты\n",
    "        loss.backward()\n",
    "        \n",
    "        # 2. Обновляем параметры сети\n",
    "        opt.step()\n",
    "        \n",
    "        # 3. Обнуляем посчитанные градиенты параметров. Забыть про это — частая ошибка!\n",
    "        opt.zero_grad()\n",
    "        #################################################################################\n",
    "\n",
    "    # Валидация на каждой второй эпохе\n",
    "    if epoch % 2 == 0:\n",
    "        valid_losses = [] # сюда будем складывать средний лосс по батчам\n",
    "        valid_accuracies = []\n",
    "        # мы считаем качество, поэтому мы запрещаем фреймворку считать градиенты по параметрам\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in tqdm(valid_dataloader, desc=f'Epoch {epoch} | Valid'):\n",
    "                y_pred = model(x_batch) # делаем предсказания\n",
    "                loss = criterion(y_pred, y_batch) # считаем лосс\n",
    "                valid_losses.append(loss.numpy()) # добавляем в массив\n",
    "                valid_accuracies.extend((torch.argmax(y_pred, dim=-1) == y_batch).numpy().tolist())\n",
    "\n",
    "        # выводим статистику\n",
    "        valid_accuracy = np.mean(valid_accuracies)\n",
    "        print(f'Epoch: {epoch}, loss: {np.mean(valid_losses):.5f}, accuracy: {valid_accuracy}')\n",
    "        if valid_accuracy > 0.975:\n",
    "            print('Done!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что эта модель предсказывает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10\n",
    "cols = 10\n",
    "\n",
    "f, axarr = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        idx = i * cols + j\n",
    "        axarr[i, j].imshow(mnist_valid[idx][0].squeeze(0).numpy().reshape([28, 28]), cmap='gray')\n",
    "        y_true = mnist_valid[idx][1]\n",
    "        y_pred = torch.argmax(model(mnist_valid[idx][0]).squeeze(0), dim=-1).numpy()\n",
    "        axarr[i, j].set_title(f'{y_true} | {y_pred}', color='black' if y_true == y_pred else 'red')\n",
    "\n",
    "for ax in f.axes:\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "f.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные материалы:\n",
    "\n",
    "* [PyTorch за 60 минут](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [Использование PyTorch на GPU](https://pytorch.org/docs/master/notes/cuda.html)\n",
    "* [Хорошая книга про PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "Этот ноутбук основан на [ноутбуке](https://github.com/hse-ds/iad-deep-learning/blob/86313e3/sem01/sem01.ipynb) первого семинара курса по ИДА в Вышке, который, в свою очередь, основан на вводном [ноутбуке](https://github.com/yandexdataschool/Practical_DL/blob/fall20/week02_autodiff/seminar_pytorch.ipynb) второй недели курса по Deep Learning в ШАДе."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}