{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная классификация\n",
    "\n",
    "### Постановка задачи классификации\n",
    "\n",
    "Пусть задана обучающая выборка $X = \\left\\{ \\left( x_i, y_i \\right) \\right\\}_{i=1}^l, x_i \\in \\mathbb{X}, y_i \\in \\mathbb{Y},$ — $l$ пар объект-ответ, где\n",
    "$\\mathbb{X}$ — пространство объектов,\n",
    "$\\mathbb{Y}$ — пространство ответов.\n",
    "\n",
    "\n",
    "### Логистическая регрессия\n",
    "\n",
    "Рассмотрим в качестве верхней оценки пороговой функции потерь логистическую функцию:\n",
    "\n",
    "$$\\tilde{L}(M) = \\log (1 + \\exp(-M)).$$\n",
    "\n",
    "Таким образом, необходимо решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{l} \\sum_{i=1}^l \\tilde{L} (M_i) = \\frac{1}{l} \\sum_{i=1}^l \\log (1 + \\exp (-y_i \\langle w, x_i \\rangle)) \\to \\min_w$$\n",
    "\n",
    "Получившийся метод обучения называется **логистической регрессией**.\n",
    "\n",
    "Одно из полезных свойств логистической регрессии, которое будет изучено нами несколько позднее, — тот факт, что она позволяет предсказывать помимо метки класса ещё и вероятность принадлежности каждому из них, что может быть полезным в некоторых задачах.\n",
    "\n",
    "**Пример**: Вы работаете в банке и хотите выдавать кредиты только тем клиентам, которые вернут его с вероятностью не меньше 0.9.\n",
    "\n",
    "Попробуем сконструировать функцию потерь из других соображений.\n",
    "Если алгоритм $b(x) \\in [0, 1]$ действительно выдает вероятности, то\n",
    "они должны согласовываться с выборкой.\n",
    "С точки зрения алгоритма вероятность того, что в выборке встретится объект $x_i$ с классом $y_i$,\n",
    "равна $b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}$.\n",
    "\n",
    "Исходя из этого, можно записать правдоподобие выборки (т.е. вероятность получить такую выборку\n",
    "с точки зрения алгоритма):\n",
    "$$\n",
    "    Q(a, X)\n",
    "    =\n",
    "    \\prod_{i = 1}^{\\ell}\n",
    "        b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}.\n",
    "$$\n",
    "Данное правдоподобие можно использовать как функционал для обучения алгоритма --\n",
    "с той лишь оговоркой, что удобнее оптимизировать его логарифм:\n",
    "$$\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1] \\log b(x_i)\n",
    "        +\n",
    "        [y_i = -1] \\log (1 - b(x_i))\n",
    "    \\right)\n",
    "    \\to\n",
    "    \\min\n",
    "$$\n",
    "Данная функция потерь называется логарифмической (log-loss).\n",
    "\n",
    "Мы хотим предсказывать вероятности, то есть, чтобы наш алгоритм предсказывал числа в интервале [0, 1]. Этого легко достичь, если положить $b(x) = \\sigma(\\langle w, x \\rangle)$,\n",
    "где в качестве $\\sigma$ может выступать любая монотонно неубывающая функция\n",
    "с областью значений $[0, 1]$.\n",
    "Мы будем использовать сигмоидную функцию: $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "Таким образом, чем больше скалярное произведение $\\langle w, x \\rangle$,\n",
    "тем больше будет предсказанная вероятность.\n",
    "\n",
    "Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:\n",
    "\\begin{align*}\n",
    "    -\\sum_{i = 1}^{\\ell} &\\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{\\exp(-\\langle w, x_i \\rangle)}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{1}{1 + \\exp(\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log (1 + \\exp(-\\langle w, x_i \\rangle))\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log (1 + \\exp(\\langle w, x_i \\rangle))\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\log \\left(\n",
    "            1 + \\exp(-y_i \\langle w, x_i \\rangle)\n",
    "        \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Полученная функция в точности представляет собой логистические потери,\n",
    "упомянутые в начале.\n",
    "Линейная модель классификации, настроенная путём минимизации данного функционала,\n",
    "называется логистической регрессией.\n",
    "Как видно из приведенных рассуждений, она оптимизирует\n",
    "правдоподобие выборки и дает корректные оценки вероятности принадлежности к положительному классу.\n",
    "\n",
    "\n",
    "### Пример обучения логистической регрессии\n",
    "#### Определение спама по тексту электронного письма\n",
    "\n",
    "Попробуем при помощи моделей линейной классификации построить алгоритм, отделяющий спам от нормальной почты. Для экспериментов воспользуемся небольшим набором данных с [UCI](https://archive.ics.uci.edu/ml/index.php). Объекты в датасете соответствуют письмам, которые описаны признаками на основе текста письма, спам — положительный пример для классификации, хорошее письмо — отрицательный пример.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = pd.read_csv('spam_data.csv')\n",
    "spam_data\n",
    "\n",
    "X, y = spam_data.iloc[:, :-1].values, spam_data.iloc[:, -1].values\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на обучающую и тестовую в отношении 80/20 и обучим логистическую регрессию при помощи объекта [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = X[:int(len(X) * 0.8)]\n",
    "y_train = y[:int(len(X) * 0.8)]\n",
    "X_test = X[int(len(X) * 0.8):]\n",
    "y_test = y[int(len(X) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=13, solver='lbfgs', penalty='l2', tol=1e-2, max_iter=2000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим долю правильных ответов при помощи соответствующей функции из модуля [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, lr.predict(X_train)))\n",
    "print(accuracy_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чем проблема?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте выборку на train и test, используя функцию `train_test_split`. Вы должны создать переменные `X_train, X_test, y_train, y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию на полученном разбиении и сравните качество модели на train и test и рассчитайте значение `accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем смотреть на AUC-ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_train, lr.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем сделать лучше. У нашего алгоритма есть разные гиперпараметры: способ регуляризации, коэффициент регуляризации. Запустим поиск по сетке гиперпараметров, алгоритм переберет все возможные комбинации, посчитает метрику для каждого набора и выдаст лучший набор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, какая метрика будет использоваться, определяется параметром `'scoring'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя класс `GridSearchCV` проведите кросс-валидацию по коэффициенту регурелизации `C` и её виду `l1` или `'l2`. Используйте разбиение на 5 фолдов.\n",
    "В качестве оптимизируемой метреки, возьмите `roc_auc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `cv=5` говорит, что во время поиска оптимальных параметров будет использоваться кросс-валидация с 5 фолдами. Давайте вспомним, что это такое: \n",
    "\n",
    "![alt text](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "*Source: https://scikit-learn.org/stable/modules/cross_validation.html*\n",
    "\n",
    "В нашем случае, выборка будет разделена на 5 частей, и на каждой из 5 итераций часть данных будет становиться тестовой выборкой, а другая часть - обучающей. Посчитав метрики на каждой итерации, мы сможем усреднить их в конце и получить достаточно точную оценку качества нашего алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_searcher.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты лучшей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train, grid_searcher.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, grid_searcher.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полные результаты поиска гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_searcher.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший скор модели на кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем выделить лучшую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = grid_searcher.best_estimator_\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценку модели на кросс-валидации мы можем получить и без перебора гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_score = cross_val_score(lr, X_train, y_train, scoring='roc_auc', cv=5)\n",
    "print(cv_score)\n",
    "print(cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо перебора по сетке можно перебирать гиперпараметры, сгенерированные из заданного распределения. Прочитайте справку о классе `RandomizedSearchCV` и реализуйте такойже перебор гиперпараметров с помощью него, вместо `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Рассмотрим теперь другой подход к построению функции потерь,\n",
    "основанный на максимизации зазора между классами.\n",
    "Будем рассматривать линейные классификаторы вида\n",
    "$$\n",
    "    a(x) = sign (\\langle w, x \\rangle + b), \\qquad w \\in R^d, b \\in R.\n",
    "$$\n",
    "\n",
    "### Разделимый случай\n",
    "Будем считать, что существуют такие параметры $w_*$ и $b_*$,\n",
    "что соответствующий им классификатор $a(x)$ не допускает ни одной ошибки\n",
    "на обучающей выборке.\n",
    "В этом случае говорят, что выборка __линейно разделима__.\n",
    "\n",
    "Пусть задан некоторый классификатор $a(x) = sign (\\langle w, x \\rangle + b)$.\n",
    "Заметим, что если одновременно умножить параметры $w$ и $b$\n",
    "на одну и ту же положительную константу,\n",
    "то классификатор не изменится.\n",
    "Распорядимся этой свободой выбора и отнормируем параметры так, что\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:svmNormCond}\n",
    "    \\min_{x \\in X} | \\langle w, x \\rangle + b| = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Можно показать, что расстояние от произвольной точки $x_0 \\in R^d$ до гиперплоскости,\n",
    "определяемой данным классификатором, равно\n",
    "\n",
    "$$\n",
    "    \\rho(x_0, a)\n",
    "    =\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }.\n",
    "$$\n",
    "\n",
    "Тогда расстояние от гиперплоскости до ближайшего объекта обучающей выборки равно\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X}\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|} \\min_{x \\in X} |\\langle w, x \\rangle + b|\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Данная величина также называется __отступом (margin)__.\n",
    "\n",
    "Таким образом, если классификатор без ошибок разделяет обучающую выборку,\n",
    "то ширина его разделяющей полосы равна $\\frac{2}{\\|w\\|}$.\n",
    "Известно, что максимизация ширины разделяющей полосы приводит\n",
    "к повышению обобщающей способности классификатора.\n",
    "Вспомним также, что на повышение обобщающей способности направлена и регуляризация,\n",
    "которая штрафует большую норму весов -- а чем больше норма весов,\n",
    "тем меньше ширина разделяющей полосы.\n",
    "\n",
    "Итак, требуется построить классификатор, идеально разделяющий обучающую выборку,\n",
    "и при этом имеющий максимальный отступ.\n",
    "Запишем соответствующую оптимизационную задачу,\n",
    "которая и будет определять метод опорных векторов для линейно разделимой выборки (hard margin support vector machine):\n",
    "\\begin{equation}\n",
    "\\label{eq:svmSep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 \\to \\min_{w, b} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Неразделимый случай\n",
    "Рассмотрим теперь общий случай, когда выборку\n",
    "невозможно идеально разделить гиперплоскостью.\n",
    "Это означает, что какие бы $w$ и $b$ мы не взяли,\n",
    "хотя бы одно из ограничений в предыдущей задаче будет нарушено:\n",
    "\n",
    "$$\n",
    "    \\exists x_i \\in X:\\\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) < 1.\n",
    "$$\n",
    "\n",
    "Сделаем эти ограничения \"мягкими\", введя штраф $\\xi_i \\geq 0$ за их нарушение:\n",
    "\n",
    "$$\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell.\n",
    "$$\n",
    "\n",
    "Отметим, что если отступ объекта лежит между нулем и\n",
    "единицей ($0 \\leq y_i \\left( \\langle w, x_i \\rangle + b \\right) < 1$),\n",
    "то объект верно классифицируется, но имеет ненулевой штраф $\\xi > 0$.\n",
    "Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.\n",
    "\n",
    "Величина $\\frac{1}{\\|w\\|}$ в данном случае называется мягким отступом (soft margin).\n",
    "С одной стороны, мы хотим максимизировать отступ, с другой -- минимизировать\n",
    "штраф за неидеальное разделение выборки $\\sum_{i = 1}^{\\ell} \\xi_i$.\n",
    "Эти две задачи противоречат друг другу: как правило, излишняя подгонка под\n",
    "выборку приводит к маленькому отступу, и наоборот -- максимизация отступа\n",
    "приводит к большой ошибке на обучении.\n",
    "В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.\n",
    "Приходим к оптимизационной задаче,\n",
    "соответствующей методу опорных векторов для линейно неразделимой выборки (soft margin support vector machine)\n",
    "\\begin{equation}\n",
    "\\label{eq:svmUnsep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем зависимость положения разделяющей гиперплоскости в методе опорных векторов в зависимости от значения гиперпараметра $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Сгенерируем двумерную искуственную выборку из двух различных нормальных распределений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size=500\n",
    "\n",
    "mean0 = [7, 5]\n",
    "cov0 = [[4, 0], [0, 1]]  # diagonal covariance\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[4, 0], [0, 2]]\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, class_size)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, class_size)\n",
    "data = np.vstack((data0, data1))\n",
    "y = np.hstack((-np.ones(class_size), np.ones(class_size)))\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя класс `SVC` обучите классификатор с линейном ядром `kernel='linear'` и коэффициентом `C=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим получившуюся гиперплоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = SVM_classifier.coef_[0][0]\n",
    "w_2 = SVM_classifier.coef_[0][1]\n",
    "w_0 = SVM_classifier.intercept_[0]\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50, label='y = -1')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50, label='y = +1')\n",
    "#plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "colors = ['red', 'orange', 'green', 'blue', 'magenta']\n",
    "\n",
    "for i, C in enumerate([0.0001, 0.01,  1, 100, 10000]):\n",
    "    SVM_classifier = SVC(C=C, kernel='linear')\n",
    "    SVM_classifier.fit(data, y)\n",
    "    w_1 = SVM_classifier.coef_[0][0]\n",
    "    w_2 = SVM_classifier.coef_[0][1]\n",
    "    w_0 = SVM_classifier.intercept_[0]\n",
    "    plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2, color=colors[i], label='C='+str(C))\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.legend(loc=0)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметр $C$ отвечает за то, что является более приоритетным для классификатора, — \"подгонка\" под обучающую выборку или максимизация ширины разделяющей полосы.\n",
    " - При больших значениях $C$ классификатор сильно настраивается на обучение, тем самым сужая разделяющую полосу.\n",
    " - При маленьких значениях $C$ классификатор расширяет разделяющую полосу, при этом допуская ошибки на некоторых объектах обучающей выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
